{
    "text": "# 硬件结构\n\n- 冯诺依曼模型 （定义及构成）\n    - 1945年冯提出。定义计算机基本结构为 5 个部分，从中心到外部分别是：（理解成小型计算器，无储存介质）\n        \n        中央处理器(CPU)、内存、总线Bus \n        \n        输入设备、输出设备。\n        \n        ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled.png)\n        \n    \n- 32位和64位机器有什么区别\n    - *32*位只有8个, *64*位有16个*寄存器*\n    - 寻址能力\n        \n        1.64位CPU拥有更大的寻址能力，最大支持到16GB内存（了解64位系统支持多少内存，看下面介绍），而32bit只支持4G内存\n        \n    - 提取数据能力\n        \n        2.64位cpu一次性可以提取64位数据，比32位提高了一倍，理论上性能会提升一倍。\n        但这是建立在64位操作系统和64位软件的基础上的。\n        \n\n- CPU\n    \n    ref：[如何写出让 CPU 跑得更快的代码](https://www.cnblogs.com/xiaolincoding/p/13836230.html) & [缓存一致性](https://www.cnblogs.com/xiaolincoding/p/13886559.html) \n    \n    - 整个 CPU 结构分为（[分版本！！](https://nieyong.github.io/wiki_cpu/CPU%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84-%E5%AF%84%E5%AD%98%E5%99%A8.html)）\n        - CPU核心\n            - (图右边) 控制单元 存储单元 运算单元\n                - 控制单元：\n                    \n                    指令计数器 PC - 程序计数寄存器（Program Counter Register）\n                    \n                    指令寄存器\n                    \n            \n            ![CPU cache 是在内存与 CPU 之间？ 还是就是存储单元](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%201.png)\n            \n            CPU cache 是在内存与 CPU 之间？ 还是就是存储单元\n            \n            ![Untitled](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%202.png)\n            \n        - CPU Cache\n            \n            Cache 高速缓存是插在CPU寄存器和主存之间的缓存存储器\n            \n            - CPU Cache 分类：\n                \n                通常分为大小不等的三级缓存\n                \n                - L1 Cache 通常会分为「数据缓存」和「指令缓存」\n                - L2 Cache  通常是每个核心独有\n                - L3 Cache 是多个 CPU 核心共享的。`L3 Cache` 比 `L1 Cache` 和 `L2 Cache` 大很多\n                - 内存加载过程\n                    - 程序执行时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核心独有的 L2 Cache，最后进入到最快的 L1 Cache，之后才会被 CPU 读取。它们之间的层级关系，如下图：\n                        \n                        ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%203.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%203.png)\n                        \n                - 时间对比\n                    \n                    ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%204.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%204.png)\n                    \n                - 查看指令\n                    \n                    ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%205.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%205.png)\n                    \n                \n                ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%206.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%206.png)\n                \n            - CPU Cache 内部结构\n                \n                由很多个 `Cache Line` 组成\n                \n                `CPU Line` 是 CPU 从内存读取数据的基本单位，⽽ CPU Line 是由各种标志（Tag）+ 数据块（Data Block）组成\n                \n                ![CPU cache line](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%207.png)\n                \n                CPU cache line\n                \n                ![更详细的cache line](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%208.png)\n                \n                更详细的cache line\n                \n                数据会加载到CPU Cache 进行运算 (速度远快于内存)\n                \n                - CPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 **CPU Cache 中的，这样一小块一小块的数据，称为 Cache Line（缓存块）**。\n                    \n                    ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%209.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%209.png)\n                    \n            - CPU 读取过程\n                \n                根据组标记，索引，偏移量来读取内存块\n                \n                - CPU 读取数据的时候，**都是先访问 Cache，** 只有当 Cache 中找不到数据时，才会去访问内存，并把内存中的数据读⼊到 Cache 中，CPU 再从 CPU Cache 读取数据**。**\n                - **那 CPU 怎么知道要访问的内存数据是否在 Cache ⾥？**\n                    - 前提：\n                        \n                         CPU 访问内存数据时，是⼀⼩块⼀⼩块数据读取的，⼤⼩取决于 coherency_line_size 的值，⼀般 64 字节。     在内存中，这⼀块的数据我们称为内存块（Block），读取的时候我们要拿到数据所在内存块的地址。\n                        \n                    - 直接映射 Cache (Direct Mapped Cache）\n                        - 策略：\n                            - 把**内存块的地址始终「映射」在⼀个 CPU Line（缓存块） 的地址**，\n                        - 映射关系实现⽅式：\n                            - **「取模运算」**取模运算的结果就是内存块地址对应的 CPU Line（缓存块） 的地址。\n                        - 举例：内存共被划分为 32 个内存块，CPU Cache 共有 8 个 CPU Line，假设 CPU 想要 访问第 15 号内存块，如果 15 号内存块中的数据已经缓存在 CPU Line 中的话，则是⼀定映射在 7 号 CPU Line 中，因为 15 % 8 的值是 7。\n                        \n                        ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2010.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2010.png)\n                        \n                        问题：使⽤取模⽅式映射的话，就会出现多个内存块对应同⼀个 CPU Line， ⽐如上⾯的例⼦，除了 15 号内存块是映射在 7 号 CPU Line 中，还有 7 号、23 号、31 号内存块都是映射到 7 号 CPU Line 中。\n                        \n                        - 上述问题解决方法：记录信息\n                            - 组标记（Tag）\n                                \n                                这个组标记会记录当前 CPU Line 中存储的数据对应的内存块，我们可以**⽤这个组标记来区分不同的内存块**\n                                \n                                - 除了组标记信息外，**CPU Line 还有两个信息**：\n                                    - ⼀个是，从内存加载过来的实际存放**数据（Data）**。\n                                    - **有效位（Valid bit）**，它是⽤来标记对应的 CPU Line 中的数据是否是有效的，如果有效位是 0，⽆论 CPU Line 中是否有数据，CPU 都会直接访问内存，重新加载数据。\n                            - CPU Line 索引（index）\n                            - 偏移量（offset）\n                                \n                                CPU 在从 CPU Cache 读取数据的时候，并**不是读取 CPU Line 中的整个数据块，⽽是读取 CPU 所需要的⼀个数据⽚段**，这样的数据统称为⼀个字（Word）。那怎么在对应的 CPU Line 中数据块中找到所需的字呢？答，需要⼀个偏移量（Offset）。\n                                \n                            \n                            因此，⼀个**内存的访问地址**，包括**组标记、CPU Line 索引、偏移量**这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。\n                            \n                            ⽽对于 **CPU Cache ⾥的数据结构**，则是由**索引 + 有效位 + 组标记 + 数据块**组成。\n                            \n                            ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%208.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%208.png)\n                            \n                    - 如果内存中的数据已经在 CPU Cahe 中了，那 CPU 访问⼀个内存地址的时候，会经历这 4 个步骤：\n                        1. 根据内存地址中索引信息，计算在 CPU Cahe 中的索引，也就是找出对应的 CPU Line 的地址；\n                        2. 找到对应 CPU Line 后，判断 CPU Line 中的有效位，确认 CPU Line 中数据是否是有效\n                        3. 对⽐内存地址中组标记和 CPU Line 中的组标记，确认 CPU Line 中的数据是我们要访问 的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话， 则往下执⾏；\n                        4. 根据内存地址中偏移量信息，从 CPU Line 的数据块中，读取对应的字。\n                    - 除了直接映射 Cache 之外，还有 其他通过内存地址找到 CPU Cache 中的数据的策略，⽐如**全相连 Cache （Fully Associative Cache）、组相连 Cache （Set Associative Cache）等**\n    - 如何写出让 CPU 跑得更快的代码？（主要为读操作，cpu读取）\n        \n        访问的数据在 CPU Cache 中的话，意味着缓存命中，缓存命中率越⾼的话，代码的性能就会越好，CPU 也就跑的越快。这个问题，可以改成「**如何写出 CPU 缓存命 中率⾼的代码？」。**\n        \n        - 原理\n            - 缓存之所以有效，主要是因为程序运行时对内存的访问呈现[局部性（Locality）特征](https://www.notion.so/OS-Linux-socket-851919365453451c9cd9ca4a3e45b1d2)。这种局部性既包括空间局部性（Spatial Locality），也包括时间局部性（Temporal Locality）。有效利用这种局部性，缓存可以达到极高的命中率。\n        - 缓存命中率分为 「数据缓存」和「指令缓存」\n            \n            （因为CPU 就是分为 储存单元 和 控制单元 ）\n            \n            - CPU 分支预测器\n                - 对于 if 条件语句，意味着此时⾄少可 以选择跳转到两段不同的指令执⾏，也就是 if 还是 else 中的指令。那么，如果**分⽀预测可以 预测到接下来要执⾏ if ⾥的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执⾏速度就会很快。**\n                - 当数组中的元素是随机的，分⽀预测就⽆法有效⼯作，⽽当数组元素都是是顺序的，分⽀预 测器会动态地根据历史命中数据对未来进⾏预测，这样命中率就会很⾼。此，先排序再遍历速度会更快\n            - 总结：\n                - 按顺序遍历（数据缓存）\n                    - CPU Cache 是根据 CPU Cache Line 批量操作数据的\n                - 先排序在遍历（指令缓存）\n                - 如果提升多核 CPU 的缓存命中率？\n                    - 虽然 L3 Cache 是多核⼼之间共享的，但是 L1 和 L2 Cache 都是每个核⼼独有 的，如果⼀个进程在不同核⼼来回切换，各个核⼼的缓存命中率就会受到影响， 当有多个同时执⾏「计算密集型」的线程，我们可以**把线程绑定在某⼀个 CPU 核⼼上**，这样性能可以得到⾮常可观的提升。\n                    \n                    ```c\n                    // 在 Linux 上提供了 sched_setaffinity ⽅法，来实现将线程绑定到某个 CPU 核⼼这⼀功 能。\n                    \n                    #define_GNU_SOURCE\n                    #include<sched.h>\n                    int sched_setaffinity(pid_t pid,size_t cpusetsize,cpu_set_t *mask);\n                    ```\n                    \n    - 写操作\n        \n        Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不⼀致了，于是我们肯定是要把 Cache 中的数据同步到内存⾥的\n        \n        - 两种写操作\n            - 写直达（Write Through）\n                - 定义：\n                    \n                    写直达是保持内存与 Cache⼀致性最简单的⽅式：**把数据同时写⼊内存和 Cache 中。** \n                    \n                - 过程：\n                    \n                    **如果数据已经在 Cache ⾥⾯，先将数据更新到 Cache ⾥⾯，再写⼊到内存⾥⾯； 如果数据没有在 Cache ⾥⾯，就直接把数据更新到内存⾥⾯。**\n                    \n                    ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2011.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2011.png)\n                    \n                - 缺点：\n                    \n                    ⽆论数据在不在 Cache ⾥⾯，每次写操作都会 写回到内存，花费⼤量时间影响性能。\n                    \n            - 写回（Write Back）\n                \n                为了要减少数据 写回内存的频率，就出现了写回（Write Back）\n                在写回机制中，当发⽣写操作时，新的数据仅仅被写⼊ Cache Block ⾥，只有当修改过的要 Cache Block「被替换」时才需要写到内存中，减少了数据写回内存的频率，这样便可以提⾼系统的性能。\n                \n                - 具体步骤（其实看图就行）\n                    \n                    ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2012.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2012.png)\n                    \n                    ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2013.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2013.png)\n                    \n    - 缓存一致性 (和数据库很相似)\n        - 问题发⽣原因：\n            - 多核CPU & L1/L2 cache & 内存\n            - 假设 A 号核⼼和 B 号核⼼同时运⾏两个线程，都操作共同的变量 i（初始值为 0 ）。 \n            这时如果 A 号核⼼执⾏了 i++ 语句的时候，为了考虑性能，使⽤了我们前⾯所说的写回策略，**先把值为 1 的执⾏结果写⼊到 L1/L2 Cache 中**，然后把 L1/L2 Cache 中对应的 Block 标记为脏的，**这个时候数据其实没有被同步到内存中的**，因为写回策略，只有在 A 号核⼼中 的这个 Cache Block 要被替换的时候，数据才会写⼊到内存⾥。\n            **如果这时旁边的 B 号核⼼尝试从内存读取 i 变量的值，则读到的将会是错误的值，**因为刚才 A 号核⼼更新 i 值还没写⼊到内存中，内存中的值还依然是 0。\n            - 这个就是所谓的缓存⼀致性问题，A 号核⼼和 B 号核⼼的缓存，在这个时候是不⼀致，从⽽会导致执⾏结果的错误。\n            \n            ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2014.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2014.png)\n            \n        - 解决机制：\n            \n            同步两个不同核⼼⾥⾯的缓存数据**（哪两点？）**\n            \n            - **写传播（Write Propagation）：**某个 CPU 核⼼⾥的 Cache 数据更新时，必须要传播到其他核⼼的 Cache（简单不详解）\n            - **事务的串形化（Transaction Serialization）：**某个 CPU 核⼼⾥对数据的操作顺序，必须在其他核⼼看起来顺序是⼀样的（比如i=100,i =200 顺序倒了）\n                \n                假设我们有⼀个含有 4 个核⼼的 CPU，这 4 个核⼼都操作共同的变量 i（初始值为 0 ）。A 号核⼼先把 i 值变为 100，⽽此时同⼀时间，B 号核⼼先把 i 值变为 200，这⾥两个修改，都会「传播」到 C 和 D 号核⼼。\n                \n                那么问题就来了，C 号核⼼先收到了 A 号核⼼更新数据的事件，再收到 B 号核⼼更新数据的 事件，因此 C 号核⼼看到的变量 i 是先变成 100，后变成 200。 ⽽如果 D 号核⼼收到的事件是反过来的，则 D 号核⼼看到的是变量 i 先变成 200，再变成100，虽然是做到了写传播，但是各个 Cache ⾥⾯的数据还是不⼀致的。\n                \n                所以，我们要保证 C 号核⼼和 D 号核⼼都能看到相同顺序的数据变化，⽐如变量 i 都是先变 成 100，再变成 200，这样的过程就是事务的串形化。\n                \n                - 要实现事务串形化，要做到 2 点：\n                    \n                    **CPU 核⼼对于 Cache 中数据的操作，需要同步给其他 CPU 核⼼；** \n                    \n                    **要引⼊「锁」的概念，如果两个 CPU 核⼼⾥有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进⾏对应的数据更新。(和数据库思想一致)**\n                    \n            - 实现技术：\n                - 总线嗅探（负责写传播）\n                    \n                    CPU 需要每时每刻监听总线上的⼀切活动，但是不管别的 核⼼的 Cache 是否缓存相同的数据，都需要发出⼀个⼴播事件。总线嗅探只是保证了某个 CPU 核⼼的 Cache 更新数据这个事件能被其他 CPU 核⼼知 道，但是并不能保证事务串形化。\n                    \n                - MESI 协议（事务串形化，并降低总线负担）\n                    - Modified，已修改\n                    - Exclusive，独占\n                    - Shared，共享\n                    - Invalidated，已失效\n                \n    \n    - CPU 执行程序的基本过程\n        \n        1 CPU 会根据指令计数器里的内存地址，\n        \n        2 从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令⻓度自增，开始顺序读取下一条指令。\n        \n        ![Untitled](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%201.png)\n        \n        ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2015.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2015.png)\n        \n        - [EG:  a = 1 + 2 CPU执行具体过程](https://zhuanlan.zhihu.com/p/264597201)\n            \n            程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：\n            \n            - 数据 1 被存放到 0x100 位置；\n            - 数据 2 被存放到 0x104 位置；\n            \n            注意，数据和指令是分开区域存放的，存放指令区域的地方称为「正文段」。\n            \n            ![https://pic3.zhimg.com/v2-e860a0b9440e7be446706e7c824239d6_b.jpg](https://pic3.zhimg.com/v2-e860a0b9440e7be446706e7c824239d6_b.jpg)\n            \n            - (重点)**编译器会把 `a = 1 + 2` 翻译成 4 条指令，**存放到正文段中。如图，这 4 条指令被存放到了 0x200 ~ 0x20c 的区域中：\n                - 0x200 的内容是 `load` 指令将 0x100 地址中的数据 1 装入到寄存器 `R0`；\n                - 0x204 的内容是 `load` 指令将 0x104 地址中的数据 2 装入到寄存器 `R1`；\n                - 0x208 的内容是 `add` 指令将寄存器 `R0` 和 `R1` 的数据相加，并把结果存放到寄存器 `R2`；\n                - 0x20c 的内容是 `store` 指令将寄存器 `R2` 中的数据存回数据段中的 0x108 地址中，这个地址也就是变量 `a` 内存中的地址；\n                \n                编译完成后，具体执行程序的时候，程序计数器会被设置为 0x200 地址，然后依次执行这 4 条指令。\n                \n                上面的例子中，由于是在 32 位 CPU 执行的，因此一条指令是占 32 位大小，所以你会发现每条指令间隔 4 个字节。\n                \n                而数据的大小是根据你在程序中指定的变量类型，比如 `int` 类型的数据则占 4 个字节，`char`类型的数据则占 1 个字节。\n                \n    - Cache 伪共享（性能杀手）\n        - 什么是伪共享\n            - key： 两个核的cache line 出现在内存的连续区域。\n                \n                现在假设有⼀个双核⼼的 CPU，这两个 CPU 核⼼并⾏运⾏着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是类型为 long 的变量 A 和 B，这个两个数据的地址在物理内存上是连续的，如果 Cahce Line 的⼤⼩是 64 字节，并且变量 A 在 Cahce Line 的开头位置，那么这两个数据是位于同⼀个 Cache Line 中，⼜因为 CPU Line 是 CPU 从内存读取数据到 Cache 的单位，所以这两个数据会被同时读⼊到了两个 CPU 核⼼中各⾃ Cache 中。\n                \n                ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2016.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2016.png)\n                \n                - 我们来思考⼀个问题，如果这两个不同核⼼的线程分别修改不同的数据，⽐如 1 号 CPU 核⼼的线程只修改了 变量 A，或 2 号 CPU 核⼼的线程的线程只修改了变量 B，会发⽣什么呢？\n                    \n                    现在我们结合保证多核缓存一致的 MESI 协议，来说明这一整个的过程，如果你还不知道 MESI 协议，你可以看我这篇文章「[10 张图打开 CPU 缓存一致性的大门](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/PDUqwAIaUxNkbjvRfovaCg)」。\n                    \n                    ①. 最开始变量 A 和 B 都还不在 Cache 里面，假设 1 号核心绑定了线程 A，2 号核心绑定了线程 B，线程 A 只会读写变量 A，线程 B 只会读写变量 B。\n                    \n                    ![https://pic4.zhimg.com/80/v2-51b2301b0b10b574dea4dc532eee6dc3_720w.jpg](https://pic4.zhimg.com/80/v2-51b2301b0b10b574dea4dc532eee6dc3_720w.jpg)\n                    \n                    ②. 1 号核心读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和 变量 B 的数据归属于同一个 Cache Line，所以 A 和 B 的数据都会被加载到 Cache，并将此 Cache Line 标记为「独占」状态。\n                    \n                    ![https://pic2.zhimg.com/80/v2-572fcfaf7c029b1957b7ae9bf07397ad_720w.jpg](https://pic2.zhimg.com/80/v2-572fcfaf7c029b1957b7ae9bf07397ad_720w.jpg)\n                    \n                    ③. 接着，2 号核心开始从内存里读取变量 B，同样的也是读取 Cache Line 大小的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和 变量 B，此时 1 号和 2 号核心的 Cache Line 状态变为「共享」状态。\n                    \n                    ![https://pic3.zhimg.com/80/v2-c80c098b80caa70142fecf1848672f22_720w.jpg](https://pic3.zhimg.com/80/v2-c80c098b80caa70142fecf1848672f22_720w.jpg)\n                    \n                    ④. 1 号核心需要修改变量 A，发现此 Cache Line 的状态是「共享」状态，所以先需要通过总线发送消息给 2 号核心，通知 2 号核心把 Cache 中对应的 Cache Line 标记为「已失效」状态，然后 1 号核心对应的 Cache Line 状态变成「已修改」状态，并且修改变量 A。\n                    \n                    ![https://pic1.zhimg.com/80/v2-369dd0c954e7d87029ac6a5332e7711c_720w.jpg](https://pic1.zhimg.com/80/v2-369dd0c954e7d87029ac6a5332e7711c_720w.jpg)\n                    \n                    ⑤. 之后，2 号核心需要修改变量 B，此时 2 号核心的 Cache 中对应的 Cache Line 是已失效状态，另外由于 1 号核心的 Cache 也有此相同的数据，且状态为「已修改」状态，所以要先把 1 号核心的 Cache 对应的 Cache Line 写回到内存，然后 2 号核心再从内存读取 Cache Line 大小的数据到 Cache 中，最后把变量 B 修改到 2 号核心的 Cache 中，并将状态标记为「已修改」状态。\n                    \n                    ![https://pic3.zhimg.com/80/v2-7a27b4aa80c086e69ffe2afa83601c52_720w.jpg](https://pic3.zhimg.com/80/v2-7a27b4aa80c086e69ffe2afa83601c52_720w.jpg)\n                    \n                    所以，可以发现如果 1 号和 2 号 CPU 核心这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两个步骤，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属于一个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从而出现 ④ 和 ⑤ 这两个步骤。\n                    \n                    因此，这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为**伪共享（False Sharing）**。\n                    \n        - 解决方法(避免伪共享)：\n            - 对于经常修改的数据，避免在同⼀ 个 Cache Line 中。Linux 内核中存在 `__cacheline_aligned_in_smp` 宏定义，⽤于解决伪共享的问题\n                \n                ```cpp\n                #ifdef CONFIG_SMP\n                #define __cacheline_aligned_in_smp __cacheline_aligned\n                #else\n                #define __cacheline_aligned_in_smp\n                #endif\n                ```\n                \n                从上面的宏定义，我们可以看到：\n                \n                - 如果在多核（MP）系统里，该宏定义是 `__cacheline_aligned`，也就是 Cache Line 的大小；\n                - 而如果在单核系统里，该宏定义是空的；\n                \n                因此，针对在同一个 Cache Line 中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在 Cache Line 里是对齐的。\n                \n                ```cpp\n                struct test{\n                \tint a;\n                \tint b;\n                }\n                ```\n                \n                结构体里的两个成员变量 a 和 b 在物理内存地址上是连续的，于是它们可能会位于同一个 Cache Line 中，如下图：\n                \n                ![https://pic4.zhimg.com/80/v2-fcf28152b7fa66399f2b0d97c63c2f53_720w.jpg](https://pic4.zhimg.com/80/v2-fcf28152b7fa66399f2b0d97c63c2f53_720w.jpg)\n                \n                所以，为了防止前面提到的 Cache 伪共享问题，我们可以使用上面介绍的宏定义，将 b 的地址设置为 Cache Line 对齐地址，如下：\n                \n                ```cpp\n                struct test{\n                \tint a;\n                \tint b __cacheline_aligned_in_smp;\n                }\n                ```\n                \n                这样 a 和 b 变量就不会在同一个 Cache Line 中了，如下图：\n                \n                ![https://pic1.zhimg.com/80/v2-7e1731340bb1e41f8c12bf5a20e2f3b4_720w.jpg](https://pic1.zhimg.com/80/v2-7e1731340bb1e41f8c12bf5a20e2f3b4_720w.jpg)\n                \n                所以，避免 Cache 伪共享实际上是用空间换时间的思想，浪费一部分 Cache 空间，从而换来性能的提升。\n                \n    - [CPU 如何选择线程](https://zhuanlan.zhihu.com/p/276170146) (CPU调度)\n        - **答：根据优先级**\n            \n            在 Linux 内核中，进程和线程都是用 `tark_struct` (在linux中，具体quick find 关键字) 结构体表示的，区别在于线程的 `tark_struct` 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 **Linux 中的线程也被称为轻量级进程**，因为线程的 `tark_struct` 相比进程的 `tark_struct` 承载的 资源比较少，因此以「轻」得名。\n            \n            一般来说，没有创建线程的进程，是只有单个执行流，它被称为是主线程。如果想让进程处理更多的事情，可以创建多个线程分别去处理，但不管怎么样，它们对应到内核里都是 `tark_struct`。\n            \n            ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2017.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2017.png)\n            \n            所以，Linux 内核里的调度器，调度的对象就是 `tark_struct`，接下来我们就把这个数据结构统称为**任务**。\n            \n            普通任务  & 实时任务\n            \n            在 Linux 系统中，**根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越小，优先级越高**：\n            \n            - 实时任务，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在 `0~99` 范围内的就算实时任务；\n            - 普通任务，响应时间没有很高的要求，优先级在 `100~139` 范围内都是普通任务级别；\n        - 调度类\n            \n            由于任务有优先级之分，Linux 系统为了保障高优先级的任务能够尽可能早的被执行，于是分为了这几种调度类，如下图：\n            \n            ![https://pic4.zhimg.com/80/v2-d62c16c921cd7bbe943a00eb2f34de77_720w.jpg](https://pic4.zhimg.com/80/v2-d62c16c921cd7bbe943a00eb2f34de77_720w.jpg)\n            \n            - Deadline 和 Realtime 这两个调度类，都是应用于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作用如下：\n                - SCHED_DEADLINE：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度；\n                - SCHED_FIFO：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以「插队」；\n                - SCHED_RR：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；\n            - 而 Fair 调度类是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：\n                - SCHED_NORMAL：普通任务使用的调度策略；\n                - SCHED_BATCH：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。\n            \n            - 完全公平调度\n                - 我们平日里遇到的基本都是普通任务，对于普通任务来说，公平性最重要，在 Linux 里面，实现了一个基于 CFS 的调度算法，也就是**完全公平调度（Completely Fair Scheduling）**。\n                    \n                    这个算法的理念是**想让分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 vruntime，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，vruntime 是不会变化的**。\n                    \n                - 那么，**在 CFS 算法调度的时候，会优先选择 vruntime 少的任务**，以保证每个任务的公平性。\n                    \n                    这就好比，让你把一桶的奶茶平均分到 10 杯奶茶杯里，你看着哪杯奶茶少，就多倒一些；哪个多了，就先不倒，这样经过多轮操作，虽然不能保证每杯奶茶完全一样多，但至少是公平的。\n                    \n                    当然，上面提到的例子没有考虑到优先级的问题，虽然是普通任务，但是普通任务之间还是有优先级区分的，所以在计算虚拟运行时间 vruntime 还要考虑普通任务的**权重值**，注意权重值并不是优先级的值，内核中会有一个 nice 级别与权重值的转换表，nice 级别越低的权重值就越大，至于 nice 值是什么，我们后面会提到。于是就有了以下这个公式：\n                    \n                    ![https://pic2.zhimg.com/80/v2-c8d9cb54b02030ca3add0121bfebe7bd_720w.jpg](https://pic2.zhimg.com/80/v2-c8d9cb54b02030ca3add0121bfebe7bd_720w.jpg)\n                    \n                    你可以不用管 NICE_0_LOAD 是什么，你就认为它是一个常量，那么在「同样的实际运行时间」里，高权重任务的 vruntime 比低权重任务的 vruntime **少**，你可能会奇怪为什么是少的？你还记得 CFS 调度吗，它是会优先选择 vruntime 少的任务进行调度，所以高权重的任务就会被优先调度了，于是高权重的获得的实际运行时间自然就多了。\n                    \n            - CPU 运行队列\n                \n                一个系统通常都会运行着很多任务，多任务的数量基本都是远超 CPU 核心数量，因此这时候就需要**排队**。\n                \n                事实上，每个 CPU 都有自己的**运行队列（Run Queue, rq）**，用于描述在此 CPU 上所运行的所有进程，其队列包含三个运行队列，Deadline 运行队列 dl_rq、实时任务运行队列 rt_rq 和 CFS 运行队列 csf_rq，其中 csf_rq 是用红黑树来描述的，按 vruntime 大小来排序的，最左侧的叶子节点，就是下次会被调度的任务。\n                \n                ![https://pic4.zhimg.com/80/v2-d8470503a7e83a9dee3544dc0a0dc13b_720w.jpg](https://pic4.zhimg.com/80/v2-d8470503a7e83a9dee3544dc0a0dc13b_720w.jpg)\n                \n                这几种调度类是有优先级的，优先级如下：Deadline > Realtime > Fair，这意味着 Linux 选择下一个任务执行的时候，会按照此优先级顺序进行选择，也就是说先从 `dl_rq` 里选择任务，然后从 `rt_rq` 里选择任务，最后从 `csf_rq` 里选择任务。因此，**实时任务总是会比普通任务优先被执行**。\n                \n            - 调整优先级\n        \n    - 软中断\n        - 中断\n            - 定义\n                \n                在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。\n                \n            - 举例\n                \n                小林中午搬完砖，肚子饿了，点了份白切鸡外卖，这次我带闪了，没有被某团大数据大熟。虽然平台上会显示配送进度，但是我也不能一直傻傻地盯着呀，时间很宝贵，当然得去干别的事情，等外卖到了配送员会通过「电话」通知我，电话响了，我就会停下手中地事情，去拿外卖。\n                \n                这里的打电话，其实就是对应计算机里的中断，没接到电话的时候，我可以做其他的事情，只有接到了电话，也就是发生中断，我才会停下当前的事情，去进行另一个事情，也就是拿外卖。\n                \n                从这个例子，我们可以知道，**中断是一种异步的事件处理机制，可以提高系统的并发处理能力。**\n                \n            - 操作系统收到了中断请求，会打断其他进程的运行，所以**中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。**\n            - 而且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。\n                \n                还是回到外卖的例子，小林到了晚上又点起了外卖，这次为了犒劳自己，共点了两份外卖，一份小龙虾和一份奶茶，并且是由不同地配送员来配送，那么问题来了，当第一份外卖送到时，配送员给我打了长长的电话，说了一些杂七杂八的事情，比如给个好评等等，但如果这时另一位配送员也想给我打电话。\n                \n                很明显，这时第二位配送员因为我在通话中（相当于关闭了中断响应），自然就无法打通我的电话，他可能尝试了几次后就走掉了（相当于丢失了一次中断）。\n                \n        - 软中断\n            - 中断可能会出现的问题\n                \n                前面我们也提到了，中断请求的处理程序应该要短且快，这样才能减少对正常进程运行调度地影响，而且中断处理程序可能会暂时关闭中断，这时如果中断处理程序执行时间过长，**可能在还未执行完中断处理程序前，会丢失当前其他设备的中断请求**。\n                \n            - 那 Linux 系统**为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」**。\n                - **上半部用来快速处理中断**，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。\n                - **下半部用来延迟处理上半部未完成的工作**，一般以「内核线程」的方式运行。\n            - 举例1  外卖\n                \n                前面的外卖例子，由于第一个配送员长时间跟我通话，则导致第二位配送员无法拨通我的电话，其实当我接到第一位配送员的电话，可以告诉配送员说我现在下楼，剩下的事情，等我们见面再说（上半部），然后就可以挂断电话，到楼下后，在拿外卖，以及跟配送员说其他的事情（下半部）。\n                \n                这样，第一位配送员就不会占用我手机太多时间，当第二位配送员正好过来时，会有很大几率拨通我的电话。\n                \n            - 举例2  网卡接收网络包\n                \n                网卡收到网络包后，会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来响应该事件，这个事件的处理也是会分成上半部和下半部。\n                \n                上部分要做到快速处理，所以只要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态，比如把状态更新为表示数据已经读到内存中的状态值。\n                \n                接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。\n                \n            - 中断处理程序的上部分和下半部可以理解为：\n                - **上半部直接处理硬件请求，也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；\n                - **下半部是由内核触发，也就说软中断**，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；\n            \n            还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 `ksoftirqd/0`\n            \n            不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。\n            \n        - 系统里有哪些软中断？\n            - [https://zhuanlan.zhihu.com/p/338075214](https://zhuanlan.zhihu.com/p/338075214)\n    - 指令总结\n        \n        ```bash\n        #查看LICache数据缓存一次载入数据的大小\n        $ cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size\n        64\n        \n        区分下面：\n        #查看LICache数据缓存的大小\n        $cat /sys/devices/system/cpu/cpu0/cache/index0/size\n        32K\n        #查看LICache指令缓存的大小\n        $cat /sys/devices/system/cpu/cpu0/cache/indexl/size\n        32K\n        #查看L2Cache的大小\n        $cat /sys/devices/system/cpu/cpu0/cache/index2/size\n        256K\n        #查看L3Cache的大小\n        $cat /sys/devices/system/cpu/cpu0/cache/index3/size\n        3072K\n        ```\n        \n    \n- 存储器：储存器按照层次结构分为：\n    - 提纲\n        \n        ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2018.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2018.png)\n        \n        ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2019.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2019.png)\n        \n        ![Untitled](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2020.png)\n        \n        ![Untitled](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%201.png)\n        \n        ![Untitled](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2021.png)\n        \n        - 层次关系\n            \n            ![%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2022.png](%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84%20af6b2/Untitled%2022.png)\n            \n    - 寄存器\n        \n        最靠近 CPU 的控制单元和逻辑计算单元的存储器，速度是最快，价格最贵，那么数量不能很多。\n        存储器的数量通常在⼏⼗到⼏百之间，每个寄存器可以⽤来存储⼀定的字节（byte）的数 据。\n        \n        ⽐如：32 位 CPU 中⼤多数寄存器可以存储 4 个字节； 64 位 CPU 中⼤多数寄存器可以存储 8 个字节。\n        \n        寄存器的访问速度⾮常快，⼀般要求在半个 CPU 时钟周期内完成读写，CPU 时钟周期跟 CPU 主频息息相关，⽐如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。\n        \n        - 寄存器种类：? 面试考了，也太偏了\n            \n            \n        \n    - CPU Cache\n        \n        CPU Cache ⽤的是⼀种叫 SRAM（Static Random-Access Memory，静态随机存储器） 的芯⽚。\n        \n        SRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，⼀旦断电数据丢失。\n        在 SRAM ⾥⾯，⼀个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不⾼，同样 的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度⾮常快。\n        \n    - 内存 mermory\n        - 内存⽤的芯⽚和 CPU Cache 有所不同，它使⽤的是⼀种叫作 DRAM （Dynamic Random Access Memory，动态随机存取存储器） 的芯⽚。\n            \n            DRAM 存储⼀个 bit 数据，只需要⼀个晶体管和⼀个电容就能存储，但是因为数据会被存储在 电容⾥，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是\n            DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。\n            \n    - SSD/HDD 硬盘\n        \n        SSD（Solid-state disk） 就是我们常说的固体硬盘，结构和内存类似，但是它相⽐内存的优 点是断电后数据还是存在的，⽽内存、寄存器、⾼速缓存断电后数据都会丢失。内存的读写速度⽐ SSD ⼤概快 10~1000 倍。\n        \n        机械硬盘（Hard Disk Drive, HDD），它是通过物理读写 的⽅式来访问数据的，因此它访问速度是⾮常慢的，它的速度⽐内存慢 10W 倍左右。\n        由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。",
    "created": "20220317034841679",
    "creator": "Shuang",
    "title": "Draft of '硬件结构 af6b2.md' by Shuang",
    "type": "text/x-markdown",
    "tmap.id": "2534fcf9-c7de-4fa2-85d8-3a790560c31d",
    "modified": "20220317062414339",
    "modifier": "Shuang",
    "draft.title": "硬件结构 af6b2.md",
    "draft.of": "硬件结构 af6b2.md"
}